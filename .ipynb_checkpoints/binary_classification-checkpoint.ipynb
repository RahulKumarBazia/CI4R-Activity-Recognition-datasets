{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ced06d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library Loaded Successfully ..........\n",
      "Reading Dataset from PIckle Object\n",
      "(882, 80, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    import cv2\n",
    "    import os\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    print(\"Library Loaded Successfully ..........\")\n",
    "except:\n",
    "    print(\"Library not Found ! \")\n",
    "\n",
    "\n",
    "class MasterImage(object):\n",
    "\n",
    "    def __init__(self,PATH='', IMAGE_SIZE = 50):\n",
    "        self.PATH = PATH\n",
    "        self.IMAGE_SIZE = IMAGE_SIZE\n",
    "\n",
    "        self.image_data = []\n",
    "        self.x_data = []\n",
    "        self.y_data = []\n",
    "        self.CATEGORIES = []\n",
    "\n",
    "        # This will get List of categories\n",
    "        self.list_categories = []\n",
    "\n",
    "    def get_categories(self):\n",
    "        for path in os.listdir(self.PATH):\n",
    "            if '.DS_Store' in path:\n",
    "                pass\n",
    "            else:\n",
    "                self.list_categories.append(path)\n",
    "        print(\"Found Categories \",self.list_categories,'\\n')\n",
    "        return self.list_categories\n",
    "\n",
    "    def Process_Image(self):\n",
    "        try:\n",
    "            \"\"\"\n",
    "            Return Numpy array of image\n",
    "            :return: X_Data, Y_Data\n",
    "            \"\"\"\n",
    "            self.CATEGORIES = self.get_categories()\n",
    "            for categories in self.CATEGORIES:                                                  # Iterate over categories\n",
    "\n",
    "                train_folder_path = os.path.join(self.PATH, categories)                         # Folder Path\n",
    "                class_index = self.CATEGORIES.index(categories)                                 # this will get index for classification\n",
    "\n",
    "                for img in os.listdir(train_folder_path):                                       # This will iterate in the Folder\n",
    "                    new_path = os.path.join(train_folder_path, img)                             # image Path\n",
    "\n",
    "                    try:        # if any image is corrupted\n",
    "                        image_data_temp = cv2.imread(new_path,cv2.IMREAD_GRAYSCALE)                 # Read Image as numbers\n",
    "                        image_temp_resize = cv2.resize(image_data_temp,(self.IMAGE_SIZE,self.IMAGE_SIZE))\n",
    "                        self.image_data.append([image_temp_resize,class_index])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            data = np.asanyarray(self.image_data)\n",
    "\n",
    "            # Iterate over the Data\n",
    "            for x in data:\n",
    "                self.x_data.append(x[0])        # Get the X_Data\n",
    "                self.y_data.append(x[1])        # get the label\n",
    "\n",
    "            X_Data = np.asarray(self.x_data) / (255.0)      # Normalize Data\n",
    "            Y_Data = np.asarray(self.y_data)\n",
    "\n",
    "            # reshape x_Data\n",
    "\n",
    "            X_Data = X_Data.reshape(-1, self.IMAGE_SIZE, self.IMAGE_SIZE, 1)\n",
    "\n",
    "            return X_Data, Y_Data\n",
    "        except:\n",
    "            print(\"Failed to run Function Process Image \")\n",
    "\n",
    "    def pickle_image(self):\n",
    "\n",
    "        \"\"\"\n",
    "        :return: None Creates a Pickle Object of DataSet\n",
    "        \"\"\"\n",
    "        # Call the Function and Get the Data\n",
    "        X_Data,Y_Data = self.Process_Image()\n",
    "\n",
    "        # Write the Entire Data into a Pickle File\n",
    "        pickle_out = open('X_Data','wb')\n",
    "        pickle.dump(X_Data, pickle_out)\n",
    "        pickle_out.close()\n",
    "\n",
    "        # Write the Y Label Data\n",
    "        pickle_out = open('Y_Data', 'wb')\n",
    "        pickle.dump(Y_Data, pickle_out)\n",
    "        pickle_out.close()\n",
    "\n",
    "        print(\"Pickled Image Successfully \")\n",
    "        return X_Data,Y_Data\n",
    "\n",
    "    def load_dataset(self):\n",
    "\n",
    "        try:\n",
    "            # Read the Data from Pickle Object\n",
    "            X_Temp = open('X_Data','rb')\n",
    "            X_Data = pickle.load(X_Temp)\n",
    "\n",
    "            Y_Temp = open('Y_Data','rb')\n",
    "            Y_Data = pickle.load(Y_Temp)\n",
    "\n",
    "            print('Reading Dataset from PIckle Object')\n",
    "\n",
    "            return X_Data,Y_Data\n",
    "\n",
    "        except:\n",
    "            print('Could not Found Pickle File ')\n",
    "            print('Loading File and Dataset  ..........')\n",
    "\n",
    "            X_Data,Y_Data = self.pickle_image()\n",
    "            return X_Data,Y_Data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train'\n",
    "    a = MasterImage(PATH=path,\n",
    "                    IMAGE_SIZE=80)\n",
    "\n",
    "    X_Data,Y_Data = a.load_dataset()\n",
    "    print(X_Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bfa1f437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 8/16 [==============>...............] - ETA: 12s - loss: -266.6674 - accuracy: 0.0375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-a0d87da685c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_Data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation, Flatten,Conv2D,MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(150, (3, 3), input_shape=X_Data.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(75, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_Data, Y_Data, batch_size=40, epochs=100, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a98069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# plt.style.use('classic')\n",
    "# #############################################################\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "# #from keras import backend as K\n",
    "# ####################################################\n",
    "# import os\n",
    "# import cv2\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# image_directory = r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train'\n",
    "# SIZE = 150\n",
    "# dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "# label = []  #Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68524a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# walking_images = os.listdir(r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train\\Walking')\n",
    "# for i, image_name in enumerate(walking_images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
    "#     print(i, image_name)\n",
    "#     if (image_name.split('.')[1] == 'png'):\n",
    "#         image = cv2.imread(r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train\\Walking' + image_name)\n",
    "#         image = np.asarray(image)\n",
    "#         image = image.resize((SIZE, SIZE))\n",
    "#         dataset.append(np.array(image))\n",
    "#         label.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WheeledChair_images = os.listdir(r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train\\WheeledChair')\n",
    "# for i, image_name in enumerate(WheeledChair_images):\n",
    "#     if (image_name.split('.')[1] == 'png'):\n",
    "#         image = cv2.imread(r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train\\WheeledChair' + image_name)\n",
    "#         image = np.asarray(image)\n",
    "#         image = image.resize((SIZE, SIZE))\n",
    "#         dataset.append(np.array(image))\n",
    "#         label.append(0)\n",
    "\n",
    "# dataset = np.array(dataset)\n",
    "# label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# #from keras.utils import to_categorical\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.40, random_state = 0)\n",
    "\n",
    "# #Without scaling (normalize) the training may not converge. \n",
    "# #Normalization is a rescaling of the data from the original range \n",
    "# #so that all values are within the range of 0 and 1.\n",
    "# from keras.utils import normalize\n",
    "# X_train = normalize(X_train, axis=1)\n",
    "# X_test = normalize(X_test, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8db215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
