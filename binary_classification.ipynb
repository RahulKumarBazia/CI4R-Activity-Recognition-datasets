{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8c4a3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library Loaded Successfully ..........\n",
      "Reading Dataset from PIckle Object\n",
      "(882, 80, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    import cv2\n",
    "    import os\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    print(\"Library Loaded Successfully ..........\")\n",
    "except:\n",
    "    print(\"Library not Found ! \")\n",
    "\n",
    "\n",
    "class MasterImage(object):\n",
    "\n",
    "    def __init__(self,PATH='', IMAGE_SIZE = 50):\n",
    "        self.PATH = PATH\n",
    "        self.IMAGE_SIZE = IMAGE_SIZE\n",
    "\n",
    "        self.image_data = []\n",
    "        self.x_data = []\n",
    "        self.y_data = []\n",
    "        self.CATEGORIES = []\n",
    "\n",
    "        # This will get List of categories\n",
    "        self.list_categories = []\n",
    "\n",
    "    def get_categories(self):\n",
    "        for path in os.listdir(self.PATH):\n",
    "            if '.DS_Store' in path:\n",
    "                pass\n",
    "            else:\n",
    "                self.list_categories.append(path)\n",
    "        print(\"Found Categories \",self.list_categories,'\\n')\n",
    "        return self.list_categories\n",
    "\n",
    "    def Process_Image(self):\n",
    "        try:\n",
    "            \"\"\"\n",
    "            Return Numpy array of image\n",
    "            :return: X_Data, Y_Data\n",
    "            \"\"\"\n",
    "            self.CATEGORIES = self.get_categories()\n",
    "            for categories in self.CATEGORIES:                                                  # Iterate over categories\n",
    "\n",
    "                train_folder_path = os.path.join(self.PATH, categories)                         # Folder Path\n",
    "                class_index = self.CATEGORIES.index(categories)                                 # this will get index for classification\n",
    "\n",
    "                for img in os.listdir(train_folder_path):                                       # This will iterate in the Folder\n",
    "                    new_path = os.path.join(train_folder_path, img)                             # image Path\n",
    "\n",
    "                    try:        # if any image is corrupted\n",
    "                        image_data_temp = cv2.imread(new_path,cv2.IMREAD_GRAYSCALE)                 # Read Image as numbers\n",
    "                        image_temp_resize = cv2.resize(image_data_temp,(self.IMAGE_SIZE,self.IMAGE_SIZE))\n",
    "                        self.image_data.append([image_temp_resize,class_index])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            data = np.asanyarray(self.image_data)\n",
    "\n",
    "            # Iterate over the Data\n",
    "            for x in data:\n",
    "                self.x_data.append(x[0])        # Get the X_Data\n",
    "                self.y_data.append(x[1])        # get the label\n",
    "\n",
    "            X_Data = np.asarray(self.x_data) / (255.0)      # Normalize Data\n",
    "            Y_Data = np.asarray(self.y_data)\n",
    "\n",
    "            # reshape x_Data\n",
    "\n",
    "            X_Data = X_Data.reshape(-1, self.IMAGE_SIZE, self.IMAGE_SIZE, 1)\n",
    "\n",
    "            return X_Data, Y_Data\n",
    "        except:\n",
    "            print(\"Failed to run Function Process Image \")\n",
    "\n",
    "    def pickle_image(self):\n",
    "\n",
    "        \"\"\"\n",
    "        :return: None Creates a Pickle Object of DataSet\n",
    "        \"\"\"\n",
    "        # Call the Function and Get the Data\n",
    "        X_Data,Y_Data = self.Process_Image()\n",
    "\n",
    "        # Write the Entire Data into a Pickle File\n",
    "        pickle_out = open('X_Data','wb')\n",
    "        pickle.dump(X_Data, pickle_out)\n",
    "        pickle_out.close()\n",
    "\n",
    "        # Write the Y Label Data\n",
    "        pickle_out = open('Y_Data', 'wb')\n",
    "        pickle.dump(Y_Data, pickle_out)\n",
    "        pickle_out.close()\n",
    "\n",
    "        print(\"Pickled Image Successfully \")\n",
    "        return X_Data,Y_Data\n",
    "\n",
    "    def load_dataset(self):\n",
    "\n",
    "        try:\n",
    "            # Read the Data from Pickle Object\n",
    "            X_Temp = open('X_Data','rb')\n",
    "            X_Data = pickle.load(X_Temp)\n",
    "\n",
    "            Y_Temp = open('Y_Data','rb')\n",
    "            Y_Data = pickle.load(Y_Temp)\n",
    "\n",
    "            print('Reading Dataset from PIckle Object')\n",
    "\n",
    "            return X_Data,Y_Data\n",
    "\n",
    "        except:\n",
    "            print('Could not Found Pickle File ')\n",
    "            print('Loading File and Dataset  ..........')\n",
    "\n",
    "            X_Data,Y_Data = self.pickle_image()\n",
    "            return X_Data,Y_Data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train'\n",
    "    a = MasterImage(PATH=path,\n",
    "                    IMAGE_SIZE=80)\n",
    "\n",
    "    X_Data,Y_Data = a.load_dataset()\n",
    "    print(X_Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0950ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 30s 2s/step - loss: -2573.9451 - accuracy: 0.0519 - val_loss: -24005.7441 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 28s 2s/step - loss: -57323.3789 - accuracy: 0.0535 - val_loss: -299493.7188 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 28s 2s/step - loss: -400186.2188 - accuracy: 0.0535 - val_loss: -1596461.3750 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 28s 2s/step - loss: -1636444.5000 - accuracy: 0.0535 - val_loss: -5657406.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 29s 2s/step - loss: -4979533.5000 - accuracy: 0.0535 - val_loss: -15585664.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 27s 2s/step - loss: -12658949.0000 - accuracy: 0.0535 - val_loss: -36251192.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 29s 2s/step - loss: -27691128.0000 - accuracy: 0.0535 - val_loss: -74505400.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 29s 2s/step - loss: -54132260.0000 - accuracy: 0.0535 - val_loss: -139682480.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 25s 2s/step - loss: -97341672.0000 - accuracy: 0.0535 - val_loss: -243234880.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      " 8/16 [==============>...............] - ETA: 12s - loss: -146892720.0000 - accuracy: 0.0406"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation, Flatten,Conv2D,MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(150, (3, 3), input_shape=X_Data.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(75, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_Data, Y_Data, batch_size=40, epochs=100, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7584023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.style.use('classic')\n",
    "#############################################################\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "#from keras import backend as K\n",
    "####################################################\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image_directory = r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train'\n",
    "SIZE = 150\n",
    "dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "label = []  #Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "721b3888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 walking1.png\n",
      "1 walking10.png\n",
      "2 walking11.png\n",
      "3 walking12.png\n",
      "4 walking13.png\n",
      "5 walking14.png\n",
      "6 walking15.png\n",
      "7 walking16.png\n",
      "8 walking17.png\n",
      "9 walking18.png\n",
      "10 walking19.png\n",
      "11 walking2.png\n",
      "12 walking20.png\n",
      "13 walking21.png\n",
      "14 walking22.png\n",
      "15 walking23.png\n",
      "16 walking24.png\n",
      "17 walking25.png\n",
      "18 walking26.png\n",
      "19 walking27.png\n",
      "20 walking28.png\n",
      "21 walking29.png\n",
      "22 walking3.png\n",
      "23 walking30.png\n",
      "24 walking31.png\n",
      "25 walking32.png\n",
      "26 walking33.png\n",
      "27 walking34.png\n",
      "28 walking35.png\n",
      "29 walking36.png\n",
      "30 walking37.png\n",
      "31 walking38.png\n",
      "32 walking39.png\n",
      "33 walking4.png\n",
      "34 walking40.png\n",
      "35 walking41.png\n",
      "36 walking42.png\n",
      "37 walking43.png\n",
      "38 walking44.png\n",
      "39 walking45.png\n",
      "40 walking46.png\n",
      "41 walking47.png\n",
      "42 walking48.png\n",
      "43 walking49.png\n",
      "44 walking5.png\n",
      "45 walking50.png\n",
      "46 walking51.png\n",
      "47 walking52.png\n",
      "48 walking53.png\n",
      "49 walking54.png\n",
      "50 walking55.png\n",
      "51 walking56.png\n",
      "52 walking57.png\n",
      "53 walking58.png\n",
      "54 walking59.png\n",
      "55 walking6.png\n",
      "56 walking60.png\n",
      "57 walking61.png\n",
      "58 walking62.png\n",
      "59 walking63.png\n",
      "60 walking64.png\n",
      "61 walking65.png\n",
      "62 walking66.png\n",
      "63 walking67.png\n",
      "64 walking68.png\n",
      "65 walking69.png\n",
      "66 walking7.png\n",
      "67 walking70.png\n",
      "68 walking71.png\n",
      "69 walking8.png\n",
      "70 walking9.png\n"
     ]
    }
   ],
   "source": [
    "walking_images = os.listdir(r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train\\Walking')\n",
    "for i, image_name in enumerate(walking_images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
    "    print(i, image_name)\n",
    "    if (image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train\\Walking' + image_name)\n",
    "        image = np.asarray(image)\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eada9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WheeledChair_images = os.listdir(r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train\\WheeledChair')\n",
    "# for i, image_name in enumerate(WheeledChair_images):\n",
    "#     if (image_name.split('.')[1] == 'png'):\n",
    "#         image = cv2.imread(r'C:\\Users\\tjfpnc\\Documents\\GitHub\\CI4R-Activity-Recognition-datasets\\train\\WheeledChair' + image_name)\n",
    "#         image = np.asarray(image)\n",
    "#         image = image.resize((SIZE, SIZE))\n",
    "#         dataset.append(np.array(image))\n",
    "#         label.append(0)\n",
    "\n",
    "# dataset = np.array(dataset)\n",
    "# label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a16b9fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [368, 365]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-aea0dd9bbf0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#from keras.utils import to_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Without scaling (normalize) the training may not converge.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2170\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At least one array required as input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2172\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m    298\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [368, 365]"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# #from keras.utils import to_categorical\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.40, random_state = 0)\n",
    "\n",
    "# #Without scaling (normalize) the training may not converge. \n",
    "# #Normalization is a rescaling of the data from the original range \n",
    "# #so that all values are within the range of 0 and 1.\n",
    "# from keras.utils import normalize\n",
    "# X_train = normalize(X_train, axis=1)\n",
    "# X_test = normalize(X_test, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb4bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
